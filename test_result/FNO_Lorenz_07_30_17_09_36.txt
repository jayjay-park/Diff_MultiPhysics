time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 2000
num_train: 3000
num_test: 1000
num_trans: 0
iters: 6000
threshold: 0.0
batch_size: 1000
loss_type: JAC
reg_param: 0.5
num_init: 10
c: 0.0
noise: 0.01
dim: 3
T: 201
optim_name: AdamW
cotangent: FIM
dyn_sys: Lorenz
Loss Type: JAC
Batch Size: 1000
Training Loss: tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)
Test Loss: tensor(0.5549, device='cuda:0', grad_fn=<AddBackward0>)
Learned mean: tensor([ 3.0233,  0.9251, 10.5858], device='cuda:0', grad_fn=<MeanBackward1>)
True mean: tensor([ 3.9760,  3.9881, 24.2956])
MSE diff: 0.0007475853199139237
JAC diff: 0.0016825220664031804
