time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 2000
num_train: 3000
num_test: 1000
num_trans: 0
iters: 6000
threshold: 0.0
batch_size: 1000
loss_type: MSE
reg_param: 0.5
num_init: 10
c: 0.0
noise: 0.01
dim: 3
T: 201
optim_name: AdamW
cotangent: FIM
dyn_sys: Lorenz
Loss Type: MSE
Batch Size: 1000
Training Loss: tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)
Test Loss: tensor(0.6419, device='cuda:0', grad_fn=<AddBackward0>)
Learned mean: tensor([0.0384, 0.0493, 0.3986], device='cuda:0', grad_fn=<MeanBackward1>)
True mean: tensor([-3.1169, -3.1365, 24.6534])
MSE diff: 0.000839358035591431
JAC diff: 0.0
